
import os
# å¯¼å…¥é…ç½®
import sys

import cv2
import joblib
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from config import *


class WeldingDefectDetector:
    def __init__(self):
        self.model = None
        self.class_names = CLASS_NAMES
        self.scaler = StandardScaler()
        self.is_trained = False

    def preprocess_image(self, image):
        """ç®€åŒ–é¢„å¤„ç†"""
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image

        # è°ƒæ•´å°ºå¯¸
        gray = cv2.resize(gray, IMG_SIZE)

        # ç®€å•é¢„å¤„ç†
        processed = cv2.GaussianBlur(gray, (5, 5), 0)
        return processed

    def extract_simple_features(self, image):
        """æå–ç®€åŒ–ç‰¹å¾"""
        # åŸºæœ¬ç»Ÿè®¡ç‰¹å¾
        mean_val = np.mean(image)
        std_val = np.std(image)
        min_val = np.min(image)
        max_val = np.max(image)

        # è¾¹ç¼˜ç‰¹å¾
        edges = cv2.Canny(image, 100, 200)
        edge_density = np.sum(edges > 0) / edges.size

        # çº¹ç†ç‰¹å¾ï¼ˆç®€åŒ–ï¼‰
        sobelx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=5)
        sobely = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)
        gradient_magnitude = np.sqrt(sobelx ** 2 + sobely ** 2)
        gradient_mean = np.mean(gradient_magnitude)

        features = [mean_val, std_val, min_val, max_val, edge_density, gradient_mean]
        return np.array(features)

    def prepare_dataset(self, data_dir):
        """å‡†å¤‡æ•°æ®é›†"""
        print(f"ä» {data_dir} åŠ è½½æ•°æ®...")
        X = []
        y = []

        for i, class_name in enumerate(self.class_names):
            class_dir = os.path.join(data_dir, class_name)
            if not os.path.isdir(class_dir):
                print(f"âš ï¸  ç±»åˆ«æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {class_dir}")
                continue

            print(f"å¤„ç†ç±»åˆ«: {class_name}")

            image_files = [f for f in os.listdir(class_dir)
                           if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]

            if not image_files:
                print(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ°å›¾åƒæ–‡ä»¶: {class_dir}")
                continue

            for img_name in image_files:
                img_path = os.path.join(class_dir, img_name)
                image = cv2.imread(img_path)

                if image is None:
                    continue

                # æå–ç‰¹å¾
                features = self.extract_simple_features(image)
                X.append(features)
                y.append(i)

        if len(X) == 0:
            print("âŒ æ²¡æœ‰æå–åˆ°ä»»ä½•ç‰¹å¾")
            return np.array([]), np.array([])

        print(f"âœ… æˆåŠŸæå– {len(X)} ä¸ªæ ·æœ¬çš„ç‰¹å¾")
        return np.array(X), np.array(y)

    def build_model(self, input_shape, num_classes):
        """æ„å»ºç®€åŒ–æ¨¡å‹"""
        model = models.Sequential([
            layers.Dense(64, activation='relu', input_shape=(input_shape,)),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(num_classes, activation='softmax')
        ])

        model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        return model

    def train(self, data_dir, EARLY_STOPPING_PATIENCE=None, REDUCE_LR_PATIENCE=None, MIN_LEARNING_RATE=None,
              VALIDATION_SPLIT=None):
        """è®­ç»ƒæ¨¡å‹"""
        print("å¼€å§‹æå–ç‰¹å¾...")
        X, y = self.prepare_dataset(data_dir)

        if len(X) == 0:
            print("âŒ æ— æ³•è®­ç»ƒï¼šæ²¡æœ‰æ•°æ®")
            return None

        print(f"æ•°æ®é›†å½¢çŠ¶: X={X.shape}, y={y.shape}")

        # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        print(f"è®­ç»ƒé›†: {X_train.shape[0]}, æµ‹è¯•é›†: {X_test.shape[0]}")

        # æ ‡å‡†åŒ–ç‰¹å¾
        self.scaler = StandardScaler()
        X_train = self.scaler.fit_transform(X_train)
        X_test = self.scaler.transform(X_test)

        # ä¿å­˜scaler
        joblib.dump(self.scaler, SCALER_SAVE_PATH)
        print(f"âœ… ç‰¹å¾æ ‡å‡†åŒ–å™¨å·²ä¿å­˜: {SCALER_SAVE_PATH}")

        # æ„å»ºæ¨¡å‹
        print("æ„å»ºæ¨¡å‹...")
        self.model = self.build_model(X_train.shape[1], len(self.class_names))

        # è®¾ç½®å›è°ƒå‡½æ•°
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=EARLY_STOPPING_PATIENCE,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.2,
                patience=REDUCE_LR_PATIENCE,
                min_lr=MIN_LEARNING_RATE,
                verbose=1
            )
        ]

        # è®­ç»ƒæ¨¡å‹
        print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        history = self.model.fit(
            X_train, y_train,
            epochs=EPOCHS,
            batch_size=BATCH_SIZE,
            validation_split=VALIDATION_SPLIT,
            callbacks=callbacks,
            verbose=1
        )

        # è¯„ä¼°æ¨¡å‹
        print("è¯„ä¼°æ¨¡å‹...")
        test_loss, test_accuracy = self.model.evaluate(X_test, y_test, verbose=0)
        print(f"ğŸ“Š æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}")

        # ä¿å­˜æ¨¡å‹
        self.model.save(MODEL_SAVE_PATH)
        self.is_trained = True
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {MODEL_SAVE_PATH}")

        return history


if __name__ == "__main__":
    print("=" * 60)
    print("å¼€å§‹è®­ç»ƒç„Šæ¥ç¼ºé™·æ£€æµ‹æ¨¡å‹")
    print("=" * 60)

    # æ£€æŸ¥è®­ç»ƒæ•°æ®æ˜¯å¦å­˜åœ¨
    if not os.path.exists(AUGMENTED_TRAIN_DIR):
        print(f"âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {AUGMENTED_TRAIN_DIR}")
        print("è¯·å…ˆè¿è¡Œ data_preparation.py å‡†å¤‡æ•°æ®")
    else:
        detector = WeldingDefectDetector()
        try:
            history = detector.train(AUGMENTED_TRAIN_DIR)
            if history:
                print("ğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
            else:

                print("âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥")
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback

            traceback.print_exc()