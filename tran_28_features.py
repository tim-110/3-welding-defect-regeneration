import os
import cv2
import numpy as np
import joblib
import traceback  # è¡¥å……å¯¼å…¥tracebackï¼Œè§£å†³ä¹‹å‰çš„æœªå®šä¹‰é—®é¢˜
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import matplotlib.pyplot as plt
import tensorflow as tf

# å¯¼å…¥é…ç½®å’Œç‰¹å¾æå–ç±»
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from config import *  # ç¡®ä¿config.pyä¸­å®šä¹‰äº†IMG_SIZEã€CLASS_NAMESã€MODEL_SAVE_PATHã€SCALER_SAVE_PATH
from feature_extractor import WeldingFeatureExtractor  # ç¡®ä¿è¯¥ç±»æœ‰extract_all_featuresæ–¹æ³•


# -------------------------- å…³é”®ï¼šåˆ é™¤é‡å¤çš„WeldingDefectTrainerç±»ï¼Œä¿ç•™å®Œæ•´çš„ç¬¬ä¸€ä¸ªç±» --------------------------
class WeldingDefectTrainer:
    def __init__(self):
        # åˆå§‹åŒ–ç‰¹å¾æå–å™¨ï¼ˆä¸é¢„æµ‹æ—¶ä¿æŒä¸€è‡´ï¼‰
        self.feature_extractor = WeldingFeatureExtractor(img_size=IMG_SIZE)
        self.scaler = StandardScaler()  # ç”¨äºç‰¹å¾æ ‡å‡†åŒ–
        self.model = self.build_model()  # æ„å»ºæ¥æ”¶28ä¸ªç‰¹å¾çš„æ¨¡å‹

        # -------------------------- ä¸­æ–‡æ˜¾ç¤ºé…ç½®ï¼šæ”¾åœ¨ç±»åˆå§‹åŒ–ä¸­ï¼Œç¡®ä¿ç»˜å›¾å‰ç”Ÿæ•ˆ --------------------------
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']  # é€‚é…ä¸åŒç³»ç»Ÿçš„ä¸­æ–‡å­—ä½“
        plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜

    def build_model(self):
        """æ„å»ºé€‚é…28ä¸ªç‰¹å¾è¾“å…¥çš„ç¥ç»ç½‘ç»œæ¨¡å‹"""
        model = Sequential([
            # è¾“å…¥å±‚ï¼šæ˜ç¡®æ¥æ”¶28ä¸ªç‰¹å¾
            Dense(128, activation='relu', input_shape=(28,)),
            BatchNormalization(),  # åŠ é€Ÿè®­ç»ƒå¹¶é˜²æ­¢è¿‡æ‹Ÿåˆ
            Dropout(0.3),  # éšæœºä¸¢å¼ƒ30%ç¥ç»å…ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

            # éšè—å±‚
            Dense(64, activation='relu'),
            BatchNormalization(),
            Dropout(0.2),

            Dense(32, activation='relu'),
            BatchNormalization(),

            # è¾“å‡ºå±‚ï¼šæ ¹æ®ç¼ºé™·ç±»åˆ«æ•°é‡è®¾ç½®ç¥ç»å…ƒæ•°é‡ï¼ˆCLASS_NAMESä»config.pyå¯¼å…¥ï¼‰
            Dense(len(CLASS_NAMES), activation='softmax')
        ])

        # ç¼–è¯‘æ¨¡å‹ï¼ˆä½¿ç”¨sparse_categorical_crossentropyé€‚é…æ•´æ•°æ ‡ç­¾ï¼‰
        model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        return model

    def load_dataset(self, data_dir):
        """ä»æ–‡ä»¶å¤¹åŠ è½½æ•°æ®é›†å¹¶æå–28ä¸ªç‰¹å¾"""
        X = []  # ç‰¹å¾åˆ—è¡¨
        y = []  # æ ‡ç­¾åˆ—è¡¨

        # éå†æ¯ä¸ªç¼ºé™·ç±»åˆ«æ–‡ä»¶å¤¹ï¼ˆCLASS_NAMESä»config.pyå¯¼å…¥ï¼Œå¦‚['porosity', 'crack', 'spatter']ï¼‰
        for class_idx, class_name in enumerate(CLASS_NAMES):
            class_dir = os.path.join(data_dir, class_name)
            if not os.path.exists(class_dir):
                print(f"âš ï¸ ç±»åˆ«æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {class_dir}")
                continue

            # éå†æ–‡ä»¶å¤¹ä¸­çš„å›¾åƒ
            for img_name in os.listdir(class_dir):
                img_path = os.path.join(class_dir, img_name)
                # è¿‡æ»¤éå›¾åƒæ–‡ä»¶
                if not img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):
                    continue

                try:
                    # è¯»å–å›¾åƒå¹¶æå–28ä¸ªç‰¹å¾
                    image = cv2.imread(img_path)
                    if image is None:
                        print(f"âš ï¸ æ— æ³•è¯»å–å›¾åƒ: {img_path}")
                        continue

                    # è°ƒç”¨ç‰¹å¾æå–å™¨æå–28ä¸ªç‰¹å¾ï¼ˆç¡®ä¿extract_all_featuresè¿”å›é•¿åº¦ä¸º28çš„åˆ—è¡¨/æ•°ç»„ï¼‰
                    features = self.feature_extractor.extract_all_features(image)

                    # éªŒè¯ç‰¹å¾æ•°é‡æ˜¯å¦ä¸º28ï¼Œé¿å…å¼‚å¸¸ç‰¹å¾å½±å“è®­ç»ƒ
                    if len(features) != 28:
                        print(f"âš ï¸ ç‰¹å¾æ•°é‡å¼‚å¸¸ï¼ˆé¢„æœŸ28ï¼Œå®é™…{len(features)}ï¼‰: {img_path}")
                        continue

                    X.append(features)
                    y.append(class_idx)  # ç”¨æ•´æ•°è¡¨ç¤ºæ ‡ç­¾ï¼ˆé€‚é…sparse_categorical_crossentropyï¼‰

                except Exception as e:
                    print(f"âš ï¸ å¤„ç†å›¾åƒå¤±è´¥ {img_path}: {e}")
                    continue

        if not X:
            raise ValueError("âŒ æœªåŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†è·¯å¾„å’Œæ ¼å¼")

        return np.array(X), np.array(y)

    def train(self, train_dir, val_dir=None):
        """è®­ç»ƒæ¨¡å‹"""
        # 1. åŠ è½½è®­ç»ƒæ•°æ®
        print("ğŸ” åŠ è½½è®­ç»ƒæ•°æ®å¹¶æå–28ä¸ªç‰¹å¾...")
        X_train, y_train = self.load_dataset(train_dir)
        print(f"âœ… è®­ç»ƒæ•°æ®åŠ è½½å®Œæˆ: {len(X_train)}ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬28ä¸ªç‰¹å¾")

        # 2. åˆ’åˆ†éªŒè¯é›†ï¼ˆå¦‚æœæ²¡æœ‰å•ç‹¬çš„éªŒè¯é›†æ–‡ä»¶å¤¹ï¼‰
        if val_dir is None:
            X_train, X_val, y_train, y_val = train_test_split(
                X_train, y_train,
                test_size=0.2,  # 20%æ•°æ®ä½œä¸ºéªŒè¯é›†
                random_state=42,  # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
                stratify=y_train  # ä¿æŒç±»åˆ«æ¯”ä¾‹ä¸€è‡´ï¼Œé¿å…éªŒè¯é›†ç±»åˆ«å¤±è¡¡
            )
            print(f"âœ… è‡ªåŠ¨åˆ’åˆ†éªŒè¯é›†: {len(X_val)}ä¸ªæ ·æœ¬")
        else:
            # ä½¿ç”¨å•ç‹¬çš„éªŒè¯é›†æ–‡ä»¶å¤¹
            print("ğŸ” åŠ è½½éªŒè¯æ•°æ®...")
            X_val, y_val = self.load_dataset(val_dir)
            print(f"âœ… éªŒè¯æ•°æ®åŠ è½½å®Œæˆ: {len(X_val)}ä¸ªæ ·æœ¬")

        # 3. ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆä»…ç”¨è®­ç»ƒé›†æ‹Ÿåˆscalerï¼Œé¿å…æ•°æ®æ³„éœ²ï¼‰
        print("ğŸ”„ æ ‡å‡†åŒ–ç‰¹å¾...")
        self.scaler.fit(X_train)  # è®¡ç®—28ä¸ªç‰¹å¾çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆä»…ç”¨è®­ç»ƒé›†ï¼‰
        X_train_scaled = self.scaler.transform(X_train)  # æ ‡å‡†åŒ–è®­ç»ƒé›†
        X_val_scaled = self.scaler.transform(X_val)  # æ ‡å‡†åŒ–éªŒè¯é›†ï¼ˆä½¿ç”¨è®­ç»ƒé›†çš„å‡å€¼/æ ‡å‡†å·®ï¼‰

        # 4. å®šä¹‰è®­ç»ƒå›è°ƒï¼ˆæå‰åœæ­¢+ä¿å­˜æœ€ä½³æ¨¡å‹ï¼‰
        callbacks = [
            # å½“éªŒè¯é›†å‡†ç¡®ç‡è¿ç»­5è½®ä¸æå‡æ—¶åœæ­¢è®­ç»ƒï¼Œæ¢å¤æœ€ä½³æƒé‡
            EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True, verbose=1),
            # ä¿å­˜éªŒè¯é›†å‡†ç¡®ç‡æœ€é«˜çš„æ¨¡å‹ï¼ˆè·¯å¾„ä»config.pyå¯¼å…¥ï¼‰
            ModelCheckpoint(
                MODEL_SAVE_PATH,
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            )
        ]

        # 5. è®­ç»ƒæ¨¡å‹
        print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼ˆè¾“å…¥ç‰¹å¾æ•°ï¼š28ï¼‰...")
        history = self.model.fit(
            X_train_scaled, y_train,
            validation_data=(X_val_scaled, y_val),
            epochs=50,  # æœ€å¤§è®­ç»ƒè½®æ¬¡ï¼ˆEarlyStoppingä¼šæå‰åœæ­¢ï¼‰
            batch_size=16,  # æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®CPU/GPUæ€§èƒ½è°ƒæ•´ï¼‰
            callbacks=callbacks,
            verbose=1  # æ˜¾ç¤ºè®­ç»ƒè¿‡ç¨‹ï¼ˆ1=æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œ2=ä»…æ˜¾ç¤ºè½®æ¬¡ç»“æœï¼‰
        )

        # 6. ä¿å­˜æ ‡å‡†åŒ–å™¨ï¼ˆä¾›é¢„æµ‹æ—¶ä½¿ç”¨ï¼Œè·¯å¾„ä»config.pyå¯¼å…¥ï¼‰
        joblib.dump(self.scaler, SCALER_SAVE_PATH)
        print(f"âœ… ç‰¹å¾æ ‡å‡†åŒ–å™¨å·²ä¿å­˜è‡³: {SCALER_SAVE_PATH}")

        # 7. ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆè°ƒç”¨ç±»å†…æ–¹æ³•ï¼Œä¸­æ–‡å·²é…ç½®ï¼‰
        self.plot_training_history(history)

        return history  # è¿”å›è®­ç»ƒå†å²ï¼Œä¾›åç»­åˆ†æ

    def plot_training_history(self, history):
        """ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‡†ç¡®ç‡å’ŒæŸå¤±æ›²çº¿ï¼ˆä¸­æ–‡æ ‡ç­¾ï¼‰"""
        plt.figure(figsize=(12, 4))  # è®¾ç½®ç”»å¸ƒå¤§å°ï¼ˆå®½12ï¼Œé«˜4ï¼‰

        # -------------------------- å­å›¾1ï¼šå‡†ç¡®ç‡æ›²çº¿ï¼ˆä¸­æ–‡æ ‡ç­¾ï¼‰ --------------------------
        plt.subplot(1, 2, 1)  # 1è¡Œ2åˆ—ï¼Œç¬¬1ä¸ªå­å›¾
        plt.plot(history.history['accuracy'], color='#1f77b4', linewidth=2, label='è®­ç»ƒå‡†ç¡®ç‡')
        plt.plot(history.history['val_accuracy'], color='#ff7f0e', linewidth=2, label='éªŒè¯å‡†ç¡®ç‡')
        plt.title('æ¨¡å‹å‡†ç¡®ç‡å˜åŒ–', fontsize=12)  # ä¸­æ–‡æ ‡é¢˜
        plt.xlabel('è®­ç»ƒè½®æ¬¡ï¼ˆEpochï¼‰', fontsize=10)  # ä¸­æ–‡Xè½´æ ‡ç­¾
        plt.ylabel('å‡†ç¡®ç‡', fontsize=10)  # ä¸­æ–‡Yè½´æ ‡ç­¾
        plt.legend(fontsize=9)  # æ˜¾ç¤ºå›¾ä¾‹
        plt.grid(alpha=0.3)  # æ·»åŠ ç½‘æ ¼ï¼Œä¾¿äºæŸ¥çœ‹æ•°å€¼

        # -------------------------- å­å›¾2ï¼šæŸå¤±æ›²çº¿ï¼ˆä¸­æ–‡æ ‡ç­¾ï¼‰ --------------------------
        plt.subplot(1, 2, 2)  # 1è¡Œ2åˆ—ï¼Œç¬¬2ä¸ªå­å›¾
        plt.plot(history.history['loss'], color='#1f77b4', linewidth=2, label='è®­ç»ƒæŸå¤±')
        plt.plot(history.history['val_loss'], color='#ff7f0e', linewidth=2, label='éªŒè¯æŸå¤±')
        plt.title('æ¨¡å‹æŸå¤±å˜åŒ–', fontsize=12)  # ä¸­æ–‡æ ‡é¢˜
        plt.xlabel('è®­ç»ƒè½®æ¬¡ï¼ˆEpochï¼‰', fontsize=10)  # ä¸­æ–‡Xè½´æ ‡ç­¾
        plt.ylabel('æŸå¤±å€¼', fontsize=10)  # ä¸­æ–‡Yè½´æ ‡ç­¾
        plt.legend(fontsize=9)  # æ˜¾ç¤ºå›¾ä¾‹
        plt.grid(alpha=0.3)  # æ·»åŠ ç½‘æ ¼ï¼Œä¾¿äºæŸ¥çœ‹æ•°å€¼

        # è°ƒæ•´å­å›¾é—´è·ï¼Œé¿å…æ ‡ç­¾é‡å 
        plt.tight_layout()
        # ä¿å­˜å›¾åƒï¼ˆè·¯å¾„å¯è‡ªå®šä¹‰ï¼Œå¦‚éœ€è¦æŒ‡å®šè·¯å¾„å¯æ”¹ä¸ºç»å¯¹è·¯å¾„ï¼‰
        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')  # dpi=300ç¡®ä¿å›¾åƒæ¸…æ™°
        print("ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³: training_history.png")
        plt.show()  # æ˜¾ç¤ºå›¾åƒï¼ˆå¦‚æœä¸éœ€è¦æ˜¾ç¤ºï¼Œå¯æ³¨é‡Šæ‰ï¼‰


# -------------------------- ä¸»ç¨‹åºå…¥å£ --------------------------
if __name__ == "__main__":
    # åˆå§‹åŒ–è®­ç»ƒå™¨ï¼ˆåˆå§‹åŒ–æ—¶ä¼šè‡ªåŠ¨é…ç½®ä¸­æ–‡æ˜¾ç¤ºï¼‰
    trainer = WeldingDefectTrainer()

    try:
        # è®­ç»ƒæ¨¡å‹ï¼ˆæŒ‡å®šè®­ç»ƒé›†å’ŒéªŒè¯é›†è·¯å¾„ï¼‰
        history = trainer.train(
            train_dir=r"C:\Users\huanglei\PycharmProjects\PythonProject1\augmented_train_data",
            val_dir=r"C:\Users\huanglei\PycharmProjects\PythonProject1\augmented_val_data"
        )
        print("ğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        traceback.print_exc()  # æ‰“å°è¯¦ç»†é”™è¯¯å †æ ˆï¼Œä¾¿äºæ’æŸ¥é—®é¢˜  X_train_scaled, y_train,
    validation_data=(X_val_scaled, y_val),